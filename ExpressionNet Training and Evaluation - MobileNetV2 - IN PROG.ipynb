{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "import time\n",
    "import imutils\n",
    "import hashlib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, MaxPooling2D, AveragePooling2D, Dense, Activation, Dropout, Flatten, Input\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "from imutils.video import VideoStream\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "label_map = ['Anger', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "077d44e8bbb7549a09682eb09c417903bf2fd935"
   },
   "source": [
    "## Transfer Learning on MobileNet-V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 29067 images belonging to 7 classes.\n",
      "Found 3230 images belonging to 7 classes.\n",
      "Found 3589 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "image_generator = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "    rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    # randomly shift images horizontally (fraction of total width)\n",
    "    width_shift_range=0.1,\n",
    "    # randomly shift images vertically (fraction of total height)\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,  # set range for random shear\n",
    "    zoom_range=0.0,  # set range for random zoom\n",
    "    channel_shift_range=0.0,  # set range for random channel shifts\n",
    "    # set mode for filling points outside the input boundaries\n",
    "    fill_mode=\"nearest\",\n",
    "    cval=0.0,  # value used for fill_mode = \"constant\"\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False,  # randomly flip images\n",
    "    # set rescaling factor (applied before any other transformation)\n",
    "    rescale=None,\n",
    "    # set function that will be applied on each input\n",
    "    preprocessing_function=None,\n",
    "    # image data format, either \"channels_first\" or \"channels_last\"\n",
    "    data_format=\"channels_last\",\n",
    "    # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "    validation_split=0.0,\n",
    ")\n",
    "image_generator = image_generator.flow_from_directory(\n",
    "    directory='dataset/train',\n",
    "    target_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=64,\n",
    "    class_mode=\"sparse\",\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator()\n",
    "val_generator = val_generator.flow_from_directory(\n",
    "    directory='dataset/val',\n",
    "    target_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=64,\n",
    "    class_mode=\"sparse\",\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator()\n",
    "test_generator = test_generator.flow_from_directory(\n",
    "    directory='dataset/test',\n",
    "    target_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=64,\n",
    "    class_mode=\"sparse\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/khuang/miniconda3/envs/DeepSpeech/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khuang/miniconda3/envs/DeepSpeech/lib/python3.6/site-packages/keras_applications/mobilenet_v2.py:294: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  warnings.warn('`input_shape` is undefined or non-square, '\n"
     ]
    }
   ],
   "source": [
    "# load the MobileNetV2 network, ensuring the head FC layer sets are left off\n",
    "baseModel = MobileNetV2(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "# construct the head of the model that will be placed on top of the\n",
    "# the base model\n",
    "headModel = baseModel.output\n",
    "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
    "headModel = Flatten(name=\"flatten\")(headModel)\n",
    "headModel = Dense(128, activation=\"relu\")(headModel)\n",
    "headModel = Dropout(0.5)(headModel)\n",
    "headModel = Dense(len(label_map), activation=\"softmax\")(headModel)\n",
    "# place the head FC model on top of the base model (this will become\n",
    "# the actual model we will train)\n",
    "model = Model(inputs=baseModel.input, outputs=headModel)\n",
    "\n",
    "# loop over all layers in the base model and freeze them so they will\n",
    "# *not* be updated during the first training process\n",
    "# for layer in baseModel.layers:\n",
    "#     layer.trainable = False\n",
    "    \n",
    "optim = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "model.compile(loss='sparse_categorical_crossentropy', metrics=['accuracy'], optimizer=optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "455/455 [==============================] - 333s 732ms/step - loss: 1.4614 - acc: 0.4412 - val_loss: 1.7206 - val_acc: 0.3607\n",
      "Epoch 2/100\n",
      "455/455 [==============================] - 286s 628ms/step - loss: 1.1556 - acc: 0.5680 - val_loss: 1.2893 - val_acc: 0.4997\n",
      "Epoch 3/100\n",
      "455/455 [==============================] - 283s 622ms/step - loss: 1.0501 - acc: 0.6106 - val_loss: 1.5491 - val_acc: 0.4567\n",
      "Epoch 4/100\n",
      "455/455 [==============================] - 283s 622ms/step - loss: 0.9741 - acc: 0.6389 - val_loss: 1.3522 - val_acc: 0.4808\n",
      "Epoch 5/100\n",
      "455/455 [==============================] - 283s 623ms/step - loss: 0.9140 - acc: 0.6661 - val_loss: 1.1138 - val_acc: 0.6065\n",
      "Epoch 6/100\n",
      "455/455 [==============================] - 282s 620ms/step - loss: 0.8573 - acc: 0.6839 - val_loss: 1.1219 - val_acc: 0.5898\n",
      "Epoch 7/100\n",
      "455/455 [==============================] - 282s 620ms/step - loss: 0.8110 - acc: 0.7057 - val_loss: 1.1651 - val_acc: 0.5944\n",
      "Epoch 8/100\n",
      "455/455 [==============================] - 282s 621ms/step - loss: 0.7594 - acc: 0.7221 - val_loss: 1.0101 - val_acc: 0.6344\n",
      "Epoch 9/100\n",
      "455/455 [==============================] - 283s 622ms/step - loss: 0.7165 - acc: 0.7385 - val_loss: 1.0610 - val_acc: 0.6152\n",
      "Epoch 10/100\n",
      "455/455 [==============================] - 283s 622ms/step - loss: 0.6686 - acc: 0.7586 - val_loss: 1.1412 - val_acc: 0.5938\n",
      "Epoch 11/100\n",
      "455/455 [==============================] - 283s 622ms/step - loss: 0.6306 - acc: 0.7726 - val_loss: 1.0078 - val_acc: 0.6486\n",
      "Epoch 12/100\n",
      "455/455 [==============================] - 282s 621ms/step - loss: 0.5865 - acc: 0.7912 - val_loss: 1.1528 - val_acc: 0.6319\n",
      "Epoch 13/100\n",
      "455/455 [==============================] - 282s 620ms/step - loss: 0.5441 - acc: 0.8045 - val_loss: 1.0485 - val_acc: 0.6625\n",
      "Epoch 14/100\n",
      "455/455 [==============================] - 283s 621ms/step - loss: 0.5136 - acc: 0.8167 - val_loss: 1.1336 - val_acc: 0.6511\n",
      "Epoch 15/100\n",
      "455/455 [==============================] - 284s 624ms/step - loss: 0.4734 - acc: 0.8288 - val_loss: 1.1674 - val_acc: 0.6483\n",
      "Epoch 16/100\n",
      "455/455 [==============================] - 281s 619ms/step - loss: 0.4508 - acc: 0.8426 - val_loss: 1.2760 - val_acc: 0.6594\n",
      "Epoch 17/100\n",
      "455/455 [==============================] - 282s 620ms/step - loss: 0.4138 - acc: 0.8506 - val_loss: 1.1895 - val_acc: 0.6461\n",
      "Epoch 18/100\n",
      "455/455 [==============================] - 283s 621ms/step - loss: 0.3185 - acc: 0.8875 - val_loss: 1.2229 - val_acc: 0.6666\n",
      "Epoch 19/100\n",
      "455/455 [==============================] - 283s 622ms/step - loss: 0.2691 - acc: 0.9037 - val_loss: 1.2175 - val_acc: 0.6678\n",
      "Epoch 20/100\n",
      "455/455 [==============================] - 282s 620ms/step - loss: 0.2524 - acc: 0.9116 - val_loss: 1.3074 - val_acc: 0.6718\n",
      "Epoch 21/100\n",
      "455/455 [==============================] - 282s 620ms/step - loss: 0.2461 - acc: 0.9128 - val_loss: 1.3589 - val_acc: 0.6743\n",
      "Epoch 22/100\n",
      "455/455 [==============================] - 283s 622ms/step - loss: 0.2220 - acc: 0.9208 - val_loss: 1.3818 - val_acc: 0.6724\n",
      "Epoch 23/100\n",
      "455/455 [==============================] - 283s 623ms/step - loss: 0.2157 - acc: 0.9246 - val_loss: 1.4021 - val_acc: 0.6697\n",
      "Epoch 24/100\n",
      "455/455 [==============================] - 283s 621ms/step - loss: 0.1893 - acc: 0.9332 - val_loss: 1.3961 - val_acc: 0.6755\n",
      "Epoch 25/100\n",
      "455/455 [==============================] - 283s 622ms/step - loss: 0.1749 - acc: 0.9392 - val_loss: 1.4251 - val_acc: 0.6768\n",
      "Epoch 26/100\n",
      "455/455 [==============================] - 283s 622ms/step - loss: 0.1771 - acc: 0.9375 - val_loss: 1.4461 - val_acc: 0.6740\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = Path('checkpoints/mobilenet/run2/')\n",
    "checkpoint_path.mkdir(parents=True, exist_ok=True)\n",
    "path_model='checkpoints/mobilenet/run2/{epoch:02d}-{val_loss:.6f}.hdf5'\n",
    "\n",
    "h = model.fit(\n",
    "    x=image_generator, \n",
    "    steps_per_epoch=len(image_generator),\n",
    "    epochs=100, \n",
    "    verbose=1, \n",
    "    validation_data=val_generator,\n",
    "    shuffle=True,\n",
    "    use_multiprocessing=True,\n",
    "    workers=4,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(filepath=path_model),\n",
    "        EarlyStopping(patience=15),\n",
    "        ReduceLROnPlateau(patience=6, factor=0.3)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.fromarray(np.uint8(X_train_224[1,:,:,:]*255))\n",
    "im.save('test.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/khuang/miniconda3/envs/DeepSpeech/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/khuang/miniconda3/envs/DeepSpeech/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/khuang/miniconda3/envs/DeepSpeech/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "57/57 [==============================] - 8s 133ms/step - loss: 1.3393 - acc: 0.6888\n"
     ]
    }
   ],
   "source": [
    "expressionNet = load_model('checkpoints/mobilenet/run2/21-1.358947.hdf5')\n",
    "scores = expressionNet.evaluate(test_generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (expression_detector)",
   "language": "python",
   "name": "expression_detector"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
